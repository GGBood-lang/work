{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lora 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"D:\\Transformer实战\\对话机器人\\数据集\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='Langboat/bloom-389m-zh', vocab_size=42437, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-389m-zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 346,555,392 || trainable%: 0.22692822508443325\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e773cb2afca4401a905b603c9622f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3389, 'grad_norm': 0.7588899731636047, 'learning_rate': 4.985105749180816e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4545, 'grad_norm': 0.8408462405204773, 'learning_rate': 4.9702114983616324e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2744, 'grad_norm': 0.5961170196533203, 'learning_rate': 4.9553172475424484e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1914, 'grad_norm': 0.9068410396575928, 'learning_rate': 4.940422996723265e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2133, 'grad_norm': 0.9457395672798157, 'learning_rate': 4.925528745904081e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1856, 'grad_norm': 0.8803300857543945, 'learning_rate': 4.910634495084897e-05, 'epoch': 0.02}\n",
      "{'loss': 2.9602, 'grad_norm': 1.0709168910980225, 'learning_rate': 4.895740244265713e-05, 'epoch': 0.02}\n",
      "{'loss': 2.9438, 'grad_norm': 0.8969307541847229, 'learning_rate': 4.88084599344653e-05, 'epoch': 0.02}\n",
      "{'loss': 2.9198, 'grad_norm': 1.60662841796875, 'learning_rate': 4.865951742627346e-05, 'epoch': 0.03}\n",
      "{'loss': 2.8182, 'grad_norm': 0.8947272896766663, 'learning_rate': 4.851057491808162e-05, 'epoch': 0.03}\n",
      "{'loss': 2.9678, 'grad_norm': 1.20127272605896, 'learning_rate': 4.836163240988978e-05, 'epoch': 0.03}\n",
      "{'loss': 2.8381, 'grad_norm': 1.4420357942581177, 'learning_rate': 4.821268990169795e-05, 'epoch': 0.04}\n",
      "{'loss': 2.8497, 'grad_norm': 1.2843669652938843, 'learning_rate': 4.806374739350611e-05, 'epoch': 0.04}\n",
      "{'loss': 2.9421, 'grad_norm': 1.1120469570159912, 'learning_rate': 4.791480488531427e-05, 'epoch': 0.04}\n",
      "{'loss': 3.064, 'grad_norm': 3.515939235687256, 'learning_rate': 4.776586237712243e-05, 'epoch': 0.04}\n",
      "{'loss': 2.773, 'grad_norm': 0.9733248353004456, 'learning_rate': 4.761691986893059e-05, 'epoch': 0.05}\n",
      "{'loss': 2.8267, 'grad_norm': 0.9999756217002869, 'learning_rate': 4.746797736073876e-05, 'epoch': 0.05}\n",
      "{'loss': 2.8727, 'grad_norm': 1.1755818128585815, 'learning_rate': 4.731903485254692e-05, 'epoch': 0.05}\n",
      "{'loss': 2.9092, 'grad_norm': 1.0486729145050049, 'learning_rate': 4.717009234435508e-05, 'epoch': 0.06}\n",
      "{'loss': 2.7382, 'grad_norm': 1.7210297584533691, 'learning_rate': 4.702114983616324e-05, 'epoch': 0.06}\n",
      "{'loss': 2.6116, 'grad_norm': 1.4539470672607422, 'learning_rate': 4.687220732797141e-05, 'epoch': 0.06}\n",
      "{'loss': 2.6719, 'grad_norm': 0.9032914638519287, 'learning_rate': 4.672326481977957e-05, 'epoch': 0.07}\n",
      "{'loss': 3.014, 'grad_norm': 1.153471827507019, 'learning_rate': 4.657432231158773e-05, 'epoch': 0.07}\n",
      "{'loss': 2.7122, 'grad_norm': 1.8206818103790283, 'learning_rate': 4.642537980339589e-05, 'epoch': 0.07}\n",
      "{'loss': 2.8875, 'grad_norm': 1.6017467975616455, 'learning_rate': 4.627643729520406e-05, 'epoch': 0.07}\n",
      "{'loss': 2.8874, 'grad_norm': 1.5427615642547607, 'learning_rate': 4.612749478701222e-05, 'epoch': 0.08}\n",
      "{'loss': 2.9025, 'grad_norm': 3.816178560256958, 'learning_rate': 4.597855227882037e-05, 'epoch': 0.08}\n",
      "{'loss': 2.7394, 'grad_norm': 1.6611242294311523, 'learning_rate': 4.582960977062854e-05, 'epoch': 0.08}\n",
      "{'loss': 2.7685, 'grad_norm': 1.2251768112182617, 'learning_rate': 4.56806672624367e-05, 'epoch': 0.09}\n",
      "{'loss': 2.7874, 'grad_norm': 1.194219708442688, 'learning_rate': 4.553172475424487e-05, 'epoch': 0.09}\n",
      "{'loss': 2.8609, 'grad_norm': 1.3863229751586914, 'learning_rate': 4.538278224605302e-05, 'epoch': 0.09}\n",
      "{'loss': 2.7689, 'grad_norm': 1.7029705047607422, 'learning_rate': 4.523383973786119e-05, 'epoch': 0.1}\n",
      "{'loss': 2.5397, 'grad_norm': 2.110813617706299, 'learning_rate': 4.508489722966935e-05, 'epoch': 0.1}\n",
      "{'loss': 2.7751, 'grad_norm': 0.9309096336364746, 'learning_rate': 4.493595472147752e-05, 'epoch': 0.1}\n",
      "{'loss': 2.8486, 'grad_norm': 0.9521229267120361, 'learning_rate': 4.478701221328567e-05, 'epoch': 0.1}\n",
      "{'loss': 2.7697, 'grad_norm': 0.8542555570602417, 'learning_rate': 4.463806970509384e-05, 'epoch': 0.11}\n",
      "{'loss': 2.8936, 'grad_norm': 1.1196457147598267, 'learning_rate': 4.4489127196902e-05, 'epoch': 0.11}\n",
      "{'loss': 2.8589, 'grad_norm': 1.4537951946258545, 'learning_rate': 4.4340184688710166e-05, 'epoch': 0.11}\n",
      "{'loss': 2.7371, 'grad_norm': 2.009542226791382, 'learning_rate': 4.419124218051832e-05, 'epoch': 0.12}\n",
      "{'loss': 2.7988, 'grad_norm': 4.062999725341797, 'learning_rate': 4.404229967232648e-05, 'epoch': 0.12}\n",
      "{'loss': 2.7136, 'grad_norm': 1.2583738565444946, 'learning_rate': 4.389335716413465e-05, 'epoch': 0.12}\n",
      "{'loss': 2.7347, 'grad_norm': 1.7538955211639404, 'learning_rate': 4.374441465594281e-05, 'epoch': 0.13}\n",
      "{'loss': 2.8017, 'grad_norm': 1.7102501392364502, 'learning_rate': 4.359547214775097e-05, 'epoch': 0.13}\n",
      "{'loss': 2.8107, 'grad_norm': 1.8369078636169434, 'learning_rate': 4.344652963955913e-05, 'epoch': 0.13}\n",
      "{'loss': 2.6389, 'grad_norm': 1.3948404788970947, 'learning_rate': 4.32975871313673e-05, 'epoch': 0.13}\n",
      "{'loss': 2.7681, 'grad_norm': 1.4983819723129272, 'learning_rate': 4.314864462317546e-05, 'epoch': 0.14}\n",
      "{'loss': 2.7527, 'grad_norm': 1.809593915939331, 'learning_rate': 4.299970211498362e-05, 'epoch': 0.14}\n",
      "{'loss': 2.7073, 'grad_norm': 1.3124147653579712, 'learning_rate': 4.285075960679178e-05, 'epoch': 0.14}\n",
      "{'loss': 2.9919, 'grad_norm': 3.0082266330718994, 'learning_rate': 4.2701817098599946e-05, 'epoch': 0.15}\n",
      "{'loss': 2.8211, 'grad_norm': 0.7812106609344482, 'learning_rate': 4.2552874590408106e-05, 'epoch': 0.15}\n",
      "{'loss': 2.6439, 'grad_norm': 5.0854105949401855, 'learning_rate': 4.240393208221627e-05, 'epoch': 0.15}\n",
      "{'loss': 2.9505, 'grad_norm': 1.0912920236587524, 'learning_rate': 4.225498957402443e-05, 'epoch': 0.15}\n",
      "{'loss': 2.6554, 'grad_norm': 2.603084087371826, 'learning_rate': 4.2106047065832595e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7295, 'grad_norm': 2.492595672607422, 'learning_rate': 4.1957104557640756e-05, 'epoch': 0.16}\n",
      "{'loss': 2.7011, 'grad_norm': 1.7670788764953613, 'learning_rate': 4.180816204944891e-05, 'epoch': 0.16}\n",
      "{'loss': 2.6517, 'grad_norm': 2.198537588119507, 'learning_rate': 4.165921954125708e-05, 'epoch': 0.17}\n",
      "{'loss': 2.7477, 'grad_norm': 1.1939184665679932, 'learning_rate': 4.151027703306524e-05, 'epoch': 0.17}\n",
      "{'loss': 2.8634, 'grad_norm': 4.472643852233887, 'learning_rate': 4.1361334524873405e-05, 'epoch': 0.17}\n",
      "{'loss': 2.656, 'grad_norm': 1.0177243947982788, 'learning_rate': 4.121239201668156e-05, 'epoch': 0.18}\n",
      "{'loss': 2.653, 'grad_norm': 2.135883331298828, 'learning_rate': 4.1063449508489726e-05, 'epoch': 0.18}\n",
      "{'loss': 2.7966, 'grad_norm': 1.4627385139465332, 'learning_rate': 4.0914507000297886e-05, 'epoch': 0.18}\n",
      "{'loss': 2.7622, 'grad_norm': 2.273041009902954, 'learning_rate': 4.0765564492106054e-05, 'epoch': 0.18}\n",
      "{'loss': 2.8069, 'grad_norm': 1.883752465248108, 'learning_rate': 4.061662198391421e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6518, 'grad_norm': 1.1550735235214233, 'learning_rate': 4.0467679475722375e-05, 'epoch': 0.19}\n",
      "{'loss': 2.7494, 'grad_norm': 1.2603267431259155, 'learning_rate': 4.0318736967530536e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6563, 'grad_norm': 1.3310229778289795, 'learning_rate': 4.01697944593387e-05, 'epoch': 0.2}\n",
      "{'loss': 2.6713, 'grad_norm': 2.062145233154297, 'learning_rate': 4.002085195114686e-05, 'epoch': 0.2}\n",
      "{'loss': 2.769, 'grad_norm': 2.2431728839874268, 'learning_rate': 3.987190944295502e-05, 'epoch': 0.2}\n",
      "{'loss': 2.6925, 'grad_norm': 2.514852285385132, 'learning_rate': 3.9722966934763185e-05, 'epoch': 0.21}\n",
      "{'loss': 2.665, 'grad_norm': 1.2482566833496094, 'learning_rate': 3.9574024426571345e-05, 'epoch': 0.21}\n",
      "{'loss': 2.6883, 'grad_norm': 2.0312986373901367, 'learning_rate': 3.9425081918379506e-05, 'epoch': 0.21}\n",
      "{'loss': 2.715, 'grad_norm': 1.4387428760528564, 'learning_rate': 3.9276139410187666e-05, 'epoch': 0.21}\n",
      "{'loss': 2.8583, 'grad_norm': 1.8870580196380615, 'learning_rate': 3.9127196901995834e-05, 'epoch': 0.22}\n",
      "{'loss': 2.8091, 'grad_norm': 1.0892812013626099, 'learning_rate': 3.8978254393803994e-05, 'epoch': 0.22}\n",
      "{'loss': 2.6378, 'grad_norm': 1.0632234811782837, 'learning_rate': 3.8829311885612155e-05, 'epoch': 0.22}\n",
      "{'loss': 2.6641, 'grad_norm': 1.7203432321548462, 'learning_rate': 3.8680369377420316e-05, 'epoch': 0.23}\n",
      "{'loss': 2.5142, 'grad_norm': 1.4991753101348877, 'learning_rate': 3.853142686922848e-05, 'epoch': 0.23}\n",
      "{'loss': 2.7303, 'grad_norm': 2.4022951126098633, 'learning_rate': 3.8382484361036644e-05, 'epoch': 0.23}\n",
      "{'loss': 2.7689, 'grad_norm': 1.7521510124206543, 'learning_rate': 3.8233541852844804e-05, 'epoch': 0.24}\n",
      "{'loss': 2.8544, 'grad_norm': 1.485743761062622, 'learning_rate': 3.8084599344652965e-05, 'epoch': 0.24}\n",
      "{'loss': 2.864, 'grad_norm': 1.5399061441421509, 'learning_rate': 3.7935656836461125e-05, 'epoch': 0.24}\n",
      "{'loss': 2.4681, 'grad_norm': 0.7386704683303833, 'learning_rate': 3.778671432826929e-05, 'epoch': 0.24}\n",
      "{'loss': 2.7156, 'grad_norm': 1.409341812133789, 'learning_rate': 3.7637771820077446e-05, 'epoch': 0.25}\n",
      "{'loss': 2.6804, 'grad_norm': 3.7435379028320312, 'learning_rate': 3.7488829311885614e-05, 'epoch': 0.25}\n",
      "{'loss': 2.6667, 'grad_norm': 2.996455669403076, 'learning_rate': 3.7339886803693774e-05, 'epoch': 0.25}\n",
      "{'loss': 2.7435, 'grad_norm': 1.3543760776519775, 'learning_rate': 3.719094429550194e-05, 'epoch': 0.26}\n",
      "{'loss': 2.9352, 'grad_norm': 3.476573944091797, 'learning_rate': 3.7042001787310096e-05, 'epoch': 0.26}\n",
      "{'loss': 2.7309, 'grad_norm': 1.8272796869277954, 'learning_rate': 3.689305927911826e-05, 'epoch': 0.26}\n",
      "{'loss': 2.7379, 'grad_norm': 2.4224278926849365, 'learning_rate': 3.6744116770926424e-05, 'epoch': 0.27}\n",
      "{'loss': 2.6287, 'grad_norm': 1.4901788234710693, 'learning_rate': 3.659517426273459e-05, 'epoch': 0.27}\n",
      "{'loss': 2.7257, 'grad_norm': 2.1647586822509766, 'learning_rate': 3.6446231754542745e-05, 'epoch': 0.27}\n",
      "{'loss': 2.5187, 'grad_norm': 1.625123143196106, 'learning_rate': 3.629728924635091e-05, 'epoch': 0.27}\n",
      "{'loss': 2.6566, 'grad_norm': 1.5083367824554443, 'learning_rate': 3.614834673815907e-05, 'epoch': 0.28}\n",
      "{'loss': 2.7044, 'grad_norm': 2.432271718978882, 'learning_rate': 3.599940422996723e-05, 'epoch': 0.28}\n",
      "{'loss': 2.7063, 'grad_norm': 1.2830249071121216, 'learning_rate': 3.5850461721775394e-05, 'epoch': 0.28}\n",
      "{'loss': 2.6297, 'grad_norm': 1.4782336950302124, 'learning_rate': 3.5701519213583554e-05, 'epoch': 0.29}\n",
      "{'loss': 2.7026, 'grad_norm': 2.1173903942108154, 'learning_rate': 3.555257670539172e-05, 'epoch': 0.29}\n",
      "{'loss': 2.6927, 'grad_norm': 5.098506927490234, 'learning_rate': 3.540363419719988e-05, 'epoch': 0.29}\n",
      "{'loss': 2.6186, 'grad_norm': 1.9937976598739624, 'learning_rate': 3.525469168900804e-05, 'epoch': 0.29}\n",
      "{'loss': 2.5702, 'grad_norm': 3.5112342834472656, 'learning_rate': 3.5105749180816204e-05, 'epoch': 0.3}\n",
      "{'loss': 2.5824, 'grad_norm': 1.8802872896194458, 'learning_rate': 3.495680667262437e-05, 'epoch': 0.3}\n",
      "{'loss': 2.6434, 'grad_norm': 1.6083500385284424, 'learning_rate': 3.480786416443253e-05, 'epoch': 0.3}\n",
      "{'loss': 2.6537, 'grad_norm': 1.995147466659546, 'learning_rate': 3.465892165624069e-05, 'epoch': 0.31}\n",
      "{'loss': 2.6774, 'grad_norm': 1.6699833869934082, 'learning_rate': 3.450997914804885e-05, 'epoch': 0.31}\n",
      "{'loss': 2.5318, 'grad_norm': 1.9367711544036865, 'learning_rate': 3.436103663985702e-05, 'epoch': 0.31}\n",
      "{'loss': 2.7268, 'grad_norm': 1.6128363609313965, 'learning_rate': 3.421209413166518e-05, 'epoch': 0.32}\n",
      "{'loss': 2.6116, 'grad_norm': 11.118293762207031, 'learning_rate': 3.406315162347334e-05, 'epoch': 0.32}\n",
      "{'loss': 2.7927, 'grad_norm': 1.7861182689666748, 'learning_rate': 3.39142091152815e-05, 'epoch': 0.32}\n",
      "{'loss': 2.5656, 'grad_norm': 4.557783603668213, 'learning_rate': 3.376526660708966e-05, 'epoch': 0.32}\n",
      "{'loss': 2.5214, 'grad_norm': 1.652703046798706, 'learning_rate': 3.361632409889783e-05, 'epoch': 0.33}\n",
      "{'loss': 2.6164, 'grad_norm': 2.259814739227295, 'learning_rate': 3.346738159070599e-05, 'epoch': 0.33}\n",
      "{'loss': 2.4289, 'grad_norm': 1.6136085987091064, 'learning_rate': 3.331843908251415e-05, 'epoch': 0.33}\n",
      "{'loss': 2.7081, 'grad_norm': 1.9541679620742798, 'learning_rate': 3.316949657432231e-05, 'epoch': 0.34}\n",
      "{'loss': 2.6662, 'grad_norm': 4.051844596862793, 'learning_rate': 3.302055406613048e-05, 'epoch': 0.34}\n",
      "{'loss': 2.5124, 'grad_norm': 1.4767605066299438, 'learning_rate': 3.287161155793864e-05, 'epoch': 0.34}\n",
      "{'loss': 2.5553, 'grad_norm': 1.9247535467147827, 'learning_rate': 3.27226690497468e-05, 'epoch': 0.35}\n",
      "{'loss': 2.7348, 'grad_norm': 1.5785456895828247, 'learning_rate': 3.257372654155496e-05, 'epoch': 0.35}\n",
      "{'loss': 2.719, 'grad_norm': 2.736833095550537, 'learning_rate': 3.242478403336313e-05, 'epoch': 0.35}\n",
      "{'loss': 2.5693, 'grad_norm': 1.9020191431045532, 'learning_rate': 3.227584152517129e-05, 'epoch': 0.35}\n",
      "{'loss': 2.7544, 'grad_norm': 6.786728858947754, 'learning_rate': 3.212689901697944e-05, 'epoch': 0.36}\n",
      "{'loss': 2.5704, 'grad_norm': 1.9807145595550537, 'learning_rate': 3.197795650878761e-05, 'epoch': 0.36}\n",
      "{'loss': 2.6816, 'grad_norm': 2.3835155963897705, 'learning_rate': 3.182901400059577e-05, 'epoch': 0.36}\n",
      "{'loss': 2.5412, 'grad_norm': 2.419353723526001, 'learning_rate': 3.168007149240393e-05, 'epoch': 0.37}\n",
      "{'loss': 2.5948, 'grad_norm': 3.892564535140991, 'learning_rate': 3.153112898421209e-05, 'epoch': 0.37}\n",
      "{'loss': 2.6496, 'grad_norm': 2.042391777038574, 'learning_rate': 3.138218647602026e-05, 'epoch': 0.37}\n",
      "{'loss': 2.4577, 'grad_norm': 3.1964144706726074, 'learning_rate': 3.123324396782842e-05, 'epoch': 0.38}\n",
      "{'loss': 2.5727, 'grad_norm': 1.5338855981826782, 'learning_rate': 3.108430145963658e-05, 'epoch': 0.38}\n",
      "{'loss': 2.6212, 'grad_norm': 1.3210954666137695, 'learning_rate': 3.093535895144474e-05, 'epoch': 0.38}\n",
      "{'loss': 2.4988, 'grad_norm': 1.7675514221191406, 'learning_rate': 3.078641644325291e-05, 'epoch': 0.38}\n",
      "{'loss': 2.632, 'grad_norm': 2.164999008178711, 'learning_rate': 3.063747393506107e-05, 'epoch': 0.39}\n",
      "{'loss': 2.7151, 'grad_norm': 1.706931710243225, 'learning_rate': 3.0488531426869233e-05, 'epoch': 0.39}\n",
      "{'loss': 2.5578, 'grad_norm': 1.5019056797027588, 'learning_rate': 3.0339588918677393e-05, 'epoch': 0.39}\n",
      "{'loss': 2.6655, 'grad_norm': 1.6003824472427368, 'learning_rate': 3.019064641048555e-05, 'epoch': 0.4}\n",
      "{'loss': 2.666, 'grad_norm': 2.6365737915039062, 'learning_rate': 3.0041703902293718e-05, 'epoch': 0.4}\n",
      "{'loss': 2.6518, 'grad_norm': 1.556032419204712, 'learning_rate': 2.9892761394101875e-05, 'epoch': 0.4}\n",
      "{'loss': 2.6377, 'grad_norm': 3.196218252182007, 'learning_rate': 2.9743818885910042e-05, 'epoch': 0.41}\n",
      "{'loss': 2.7336, 'grad_norm': 4.376510143280029, 'learning_rate': 2.95948763777182e-05, 'epoch': 0.41}\n",
      "{'loss': 2.6607, 'grad_norm': 1.9432798624038696, 'learning_rate': 2.9445933869526367e-05, 'epoch': 0.41}\n",
      "{'loss': 2.701, 'grad_norm': 2.358030319213867, 'learning_rate': 2.9296991361334524e-05, 'epoch': 0.41}\n",
      "{'loss': 2.7259, 'grad_norm': 2.4465928077697754, 'learning_rate': 2.914804885314269e-05, 'epoch': 0.42}\n",
      "{'loss': 2.8232, 'grad_norm': 4.1000752449035645, 'learning_rate': 2.899910634495085e-05, 'epoch': 0.42}\n",
      "{'loss': 2.4635, 'grad_norm': 2.5748209953308105, 'learning_rate': 2.8850163836759013e-05, 'epoch': 0.42}\n",
      "{'loss': 2.8156, 'grad_norm': 2.7858877182006836, 'learning_rate': 2.8701221328567173e-05, 'epoch': 0.43}\n",
      "{'loss': 2.6635, 'grad_norm': 1.541054129600525, 'learning_rate': 2.8552278820375337e-05, 'epoch': 0.43}\n",
      "{'loss': 2.565, 'grad_norm': 1.9932937622070312, 'learning_rate': 2.8403336312183498e-05, 'epoch': 0.43}\n",
      "{'loss': 2.7219, 'grad_norm': 1.7881345748901367, 'learning_rate': 2.825439380399166e-05, 'epoch': 0.43}\n",
      "{'loss': 2.5604, 'grad_norm': 3.6401963233947754, 'learning_rate': 2.8105451295799822e-05, 'epoch': 0.44}\n",
      "{'loss': 2.587, 'grad_norm': 3.163327693939209, 'learning_rate': 2.7956508787607983e-05, 'epoch': 0.44}\n",
      "{'loss': 2.599, 'grad_norm': 2.126038074493408, 'learning_rate': 2.7807566279416147e-05, 'epoch': 0.44}\n",
      "{'loss': 2.6535, 'grad_norm': 2.7933664321899414, 'learning_rate': 2.7658623771224308e-05, 'epoch': 0.45}\n",
      "{'loss': 2.6669, 'grad_norm': 2.609753131866455, 'learning_rate': 2.750968126303247e-05, 'epoch': 0.45}\n",
      "{'loss': 2.685, 'grad_norm': 2.1514244079589844, 'learning_rate': 2.7360738754840632e-05, 'epoch': 0.45}\n",
      "{'loss': 2.7619, 'grad_norm': 1.4445252418518066, 'learning_rate': 2.7211796246648796e-05, 'epoch': 0.46}\n",
      "{'loss': 2.6368, 'grad_norm': 1.5669714212417603, 'learning_rate': 2.7062853738456957e-05, 'epoch': 0.46}\n",
      "{'loss': 2.8458, 'grad_norm': 2.264296054840088, 'learning_rate': 2.691391123026512e-05, 'epoch': 0.46}\n",
      "{'loss': 2.6847, 'grad_norm': 1.2101925611495972, 'learning_rate': 2.676496872207328e-05, 'epoch': 0.46}\n",
      "{'loss': 2.5351, 'grad_norm': 2.047083616256714, 'learning_rate': 2.6616026213881445e-05, 'epoch': 0.47}\n",
      "{'loss': 2.4243, 'grad_norm': 2.227478265762329, 'learning_rate': 2.6467083705689606e-05, 'epoch': 0.47}\n",
      "{'loss': 2.5378, 'grad_norm': 1.7586333751678467, 'learning_rate': 2.631814119749777e-05, 'epoch': 0.47}\n",
      "{'loss': 2.6686, 'grad_norm': 1.9696712493896484, 'learning_rate': 2.616919868930593e-05, 'epoch': 0.48}\n",
      "{'loss': 2.6912, 'grad_norm': 2.374162435531616, 'learning_rate': 2.6020256181114088e-05, 'epoch': 0.48}\n",
      "{'loss': 2.4903, 'grad_norm': 3.073307752609253, 'learning_rate': 2.5871313672922255e-05, 'epoch': 0.48}\n",
      "{'loss': 2.6323, 'grad_norm': 1.2731603384017944, 'learning_rate': 2.5722371164730412e-05, 'epoch': 0.49}\n",
      "{'loss': 2.4529, 'grad_norm': 1.4851943254470825, 'learning_rate': 2.557342865653858e-05, 'epoch': 0.49}\n",
      "{'loss': 2.7105, 'grad_norm': 1.6085971593856812, 'learning_rate': 2.5424486148346737e-05, 'epoch': 0.49}\n",
      "{'loss': 2.7383, 'grad_norm': 3.0726478099823, 'learning_rate': 2.5275543640154904e-05, 'epoch': 0.49}\n",
      "{'loss': 2.6027, 'grad_norm': 2.1920979022979736, 'learning_rate': 2.512660113196306e-05, 'epoch': 0.5}\n",
      "{'loss': 2.7377, 'grad_norm': 1.244787573814392, 'learning_rate': 2.4977658623771225e-05, 'epoch': 0.5}\n",
      "{'loss': 2.7155, 'grad_norm': 1.435895323753357, 'learning_rate': 2.4828716115579386e-05, 'epoch': 0.5}\n",
      "{'loss': 2.4371, 'grad_norm': 2.2679483890533447, 'learning_rate': 2.467977360738755e-05, 'epoch': 0.51}\n",
      "{'loss': 2.5514, 'grad_norm': 5.357762336730957, 'learning_rate': 2.453083109919571e-05, 'epoch': 0.51}\n",
      "{'loss': 2.4498, 'grad_norm': 2.119832992553711, 'learning_rate': 2.4381888591003874e-05, 'epoch': 0.51}\n",
      "{'loss': 2.6416, 'grad_norm': 2.0738723278045654, 'learning_rate': 2.4232946082812035e-05, 'epoch': 0.52}\n",
      "{'loss': 2.5115, 'grad_norm': 1.5788319110870361, 'learning_rate': 2.40840035746202e-05, 'epoch': 0.52}\n",
      "{'loss': 2.6271, 'grad_norm': 1.934470295906067, 'learning_rate': 2.393506106642836e-05, 'epoch': 0.52}\n",
      "{'loss': 2.5858, 'grad_norm': 2.921755790710449, 'learning_rate': 2.3786118558236524e-05, 'epoch': 0.52}\n",
      "{'loss': 2.6496, 'grad_norm': 3.459118127822876, 'learning_rate': 2.3637176050044684e-05, 'epoch': 0.53}\n",
      "{'loss': 2.5206, 'grad_norm': 1.9469597339630127, 'learning_rate': 2.3488233541852848e-05, 'epoch': 0.53}\n",
      "{'loss': 2.5812, 'grad_norm': 1.5721515417099, 'learning_rate': 2.333929103366101e-05, 'epoch': 0.53}\n",
      "{'loss': 2.467, 'grad_norm': 1.5590989589691162, 'learning_rate': 2.319034852546917e-05, 'epoch': 0.54}\n",
      "{'loss': 2.6436, 'grad_norm': 2.2254831790924072, 'learning_rate': 2.304140601727733e-05, 'epoch': 0.54}\n",
      "{'loss': 2.7651, 'grad_norm': 1.7418098449707031, 'learning_rate': 2.2892463509085494e-05, 'epoch': 0.54}\n",
      "{'loss': 2.6762, 'grad_norm': 1.5052205324172974, 'learning_rate': 2.2743521000893654e-05, 'epoch': 0.55}\n",
      "{'loss': 2.642, 'grad_norm': 2.126441240310669, 'learning_rate': 2.259457849270182e-05, 'epoch': 0.55}\n",
      "{'loss': 2.6156, 'grad_norm': 2.0225088596343994, 'learning_rate': 2.244563598450998e-05, 'epoch': 0.55}\n",
      "{'loss': 2.7737, 'grad_norm': 2.5266976356506348, 'learning_rate': 2.2296693476318143e-05, 'epoch': 0.55}\n",
      "{'loss': 2.6898, 'grad_norm': 2.6710617542266846, 'learning_rate': 2.2147750968126304e-05, 'epoch': 0.56}\n",
      "{'loss': 2.7166, 'grad_norm': 1.925458550453186, 'learning_rate': 2.1998808459934468e-05, 'epoch': 0.56}\n",
      "{'loss': 2.7203, 'grad_norm': 2.4139184951782227, 'learning_rate': 2.1849865951742628e-05, 'epoch': 0.56}\n",
      "{'loss': 2.6766, 'grad_norm': 3.1294217109680176, 'learning_rate': 2.1700923443550792e-05, 'epoch': 0.57}\n",
      "{'loss': 2.6211, 'grad_norm': 1.5809471607208252, 'learning_rate': 2.1551980935358953e-05, 'epoch': 0.57}\n",
      "{'loss': 2.5838, 'grad_norm': 1.7904325723648071, 'learning_rate': 2.1403038427167117e-05, 'epoch': 0.57}\n",
      "{'loss': 2.6095, 'grad_norm': 5.993839263916016, 'learning_rate': 2.1254095918975274e-05, 'epoch': 0.57}\n",
      "{'loss': 2.5771, 'grad_norm': 1.6516265869140625, 'learning_rate': 2.1105153410783438e-05, 'epoch': 0.58}\n",
      "{'loss': 2.5553, 'grad_norm': 1.8734886646270752, 'learning_rate': 2.09562109025916e-05, 'epoch': 0.58}\n",
      "{'loss': 2.6454, 'grad_norm': 2.8397371768951416, 'learning_rate': 2.0807268394399762e-05, 'epoch': 0.58}\n",
      "{'loss': 2.5244, 'grad_norm': 1.815185785293579, 'learning_rate': 2.0658325886207923e-05, 'epoch': 0.59}\n",
      "{'loss': 2.6604, 'grad_norm': 0.9629529714584351, 'learning_rate': 2.0509383378016087e-05, 'epoch': 0.59}\n",
      "{'loss': 2.616, 'grad_norm': 2.2032113075256348, 'learning_rate': 2.0360440869824248e-05, 'epoch': 0.59}\n",
      "{'loss': 2.6772, 'grad_norm': 4.670863628387451, 'learning_rate': 2.021149836163241e-05, 'epoch': 0.6}\n",
      "{'loss': 2.563, 'grad_norm': 3.8057820796966553, 'learning_rate': 2.0062555853440572e-05, 'epoch': 0.6}\n",
      "{'loss': 2.5673, 'grad_norm': 2.4257020950317383, 'learning_rate': 1.9913613345248736e-05, 'epoch': 0.6}\n",
      "{'loss': 2.6545, 'grad_norm': 1.816114068031311, 'learning_rate': 1.9764670837056897e-05, 'epoch': 0.6}\n",
      "{'loss': 2.5106, 'grad_norm': 1.5127280950546265, 'learning_rate': 1.961572832886506e-05, 'epoch': 0.61}\n",
      "{'loss': 2.6904, 'grad_norm': 1.8417911529541016, 'learning_rate': 1.946678582067322e-05, 'epoch': 0.61}\n",
      "{'loss': 2.5915, 'grad_norm': 2.5814380645751953, 'learning_rate': 1.9317843312481382e-05, 'epoch': 0.61}\n",
      "{'loss': 2.4674, 'grad_norm': 2.2739500999450684, 'learning_rate': 1.9168900804289542e-05, 'epoch': 0.62}\n",
      "{'loss': 2.5891, 'grad_norm': 2.3222811222076416, 'learning_rate': 1.9019958296097706e-05, 'epoch': 0.62}\n",
      "{'loss': 2.469, 'grad_norm': 2.8479371070861816, 'learning_rate': 1.8871015787905867e-05, 'epoch': 0.62}\n",
      "{'loss': 2.551, 'grad_norm': 2.1458287239074707, 'learning_rate': 1.872207327971403e-05, 'epoch': 0.63}\n",
      "{'loss': 2.687, 'grad_norm': 1.9361079931259155, 'learning_rate': 1.857313077152219e-05, 'epoch': 0.63}\n",
      "{'loss': 2.6051, 'grad_norm': 3.9361534118652344, 'learning_rate': 1.8424188263330356e-05, 'epoch': 0.63}\n",
      "{'loss': 2.6315, 'grad_norm': 1.5529048442840576, 'learning_rate': 1.8275245755138516e-05, 'epoch': 0.63}\n",
      "{'loss': 2.4605, 'grad_norm': 1.793357253074646, 'learning_rate': 1.812630324694668e-05, 'epoch': 0.64}\n",
      "{'loss': 2.4842, 'grad_norm': 2.143862247467041, 'learning_rate': 1.797736073875484e-05, 'epoch': 0.64}\n",
      "{'loss': 2.7064, 'grad_norm': 1.3826730251312256, 'learning_rate': 1.7828418230563005e-05, 'epoch': 0.64}\n",
      "{'loss': 2.4832, 'grad_norm': 1.5394954681396484, 'learning_rate': 1.7679475722371165e-05, 'epoch': 0.65}\n",
      "{'loss': 2.3982, 'grad_norm': 2.142920732498169, 'learning_rate': 1.753053321417933e-05, 'epoch': 0.65}\n",
      "{'loss': 2.69, 'grad_norm': 3.6188130378723145, 'learning_rate': 1.738159070598749e-05, 'epoch': 0.65}\n",
      "{'loss': 2.4536, 'grad_norm': 2.7647273540496826, 'learning_rate': 1.723264819779565e-05, 'epoch': 0.66}\n",
      "{'loss': 2.5931, 'grad_norm': 2.8632500171661377, 'learning_rate': 1.7083705689603814e-05, 'epoch': 0.66}\n",
      "{'loss': 2.5139, 'grad_norm': 1.9039862155914307, 'learning_rate': 1.6934763181411975e-05, 'epoch': 0.66}\n",
      "{'loss': 2.4533, 'grad_norm': 1.4047322273254395, 'learning_rate': 1.6785820673220136e-05, 'epoch': 0.66}\n",
      "{'loss': 2.5855, 'grad_norm': 1.9063003063201904, 'learning_rate': 1.66368781650283e-05, 'epoch': 0.67}\n",
      "{'loss': 2.594, 'grad_norm': 3.1418347358703613, 'learning_rate': 1.648793565683646e-05, 'epoch': 0.67}\n",
      "{'loss': 2.4914, 'grad_norm': 4.743220329284668, 'learning_rate': 1.6338993148644624e-05, 'epoch': 0.67}\n",
      "{'loss': 2.5897, 'grad_norm': 4.538662910461426, 'learning_rate': 1.6190050640452785e-05, 'epoch': 0.68}\n",
      "{'loss': 2.722, 'grad_norm': 2.3125765323638916, 'learning_rate': 1.604110813226095e-05, 'epoch': 0.68}\n",
      "{'loss': 2.6373, 'grad_norm': 4.360415935516357, 'learning_rate': 1.589216562406911e-05, 'epoch': 0.68}\n",
      "{'loss': 2.6018, 'grad_norm': 2.0121467113494873, 'learning_rate': 1.5743223115877273e-05, 'epoch': 0.69}\n",
      "{'loss': 2.5543, 'grad_norm': 2.276883840560913, 'learning_rate': 1.5594280607685434e-05, 'epoch': 0.69}\n",
      "{'loss': 2.5313, 'grad_norm': 2.065152406692505, 'learning_rate': 1.5445338099493598e-05, 'epoch': 0.69}\n",
      "{'loss': 2.666, 'grad_norm': 1.3389040231704712, 'learning_rate': 1.529639559130176e-05, 'epoch': 0.69}\n",
      "{'loss': 2.6655, 'grad_norm': 2.381498098373413, 'learning_rate': 1.5147453083109919e-05, 'epoch': 0.7}\n",
      "{'loss': 2.6633, 'grad_norm': 2.3402249813079834, 'learning_rate': 1.4998510574918081e-05, 'epoch': 0.7}\n",
      "{'loss': 2.6801, 'grad_norm': 3.7867462635040283, 'learning_rate': 1.4849568066726244e-05, 'epoch': 0.7}\n",
      "{'loss': 2.5085, 'grad_norm': 2.3433997631073, 'learning_rate': 1.4700625558534406e-05, 'epoch': 0.71}\n",
      "{'loss': 2.4199, 'grad_norm': 3.8093583583831787, 'learning_rate': 1.4551683050342568e-05, 'epoch': 0.71}\n",
      "{'loss': 2.6172, 'grad_norm': 1.8283690214157104, 'learning_rate': 1.440274054215073e-05, 'epoch': 0.71}\n",
      "{'loss': 2.6171, 'grad_norm': 2.845147132873535, 'learning_rate': 1.4253798033958893e-05, 'epoch': 0.71}\n",
      "{'loss': 2.6688, 'grad_norm': 1.7918132543563843, 'learning_rate': 1.4104855525767055e-05, 'epoch': 0.72}\n",
      "{'loss': 2.7115, 'grad_norm': 2.9282937049865723, 'learning_rate': 1.3955913017575217e-05, 'epoch': 0.72}\n",
      "{'loss': 2.625, 'grad_norm': 1.3303942680358887, 'learning_rate': 1.380697050938338e-05, 'epoch': 0.72}\n",
      "{'loss': 2.6314, 'grad_norm': 3.998948812484741, 'learning_rate': 1.3658028001191542e-05, 'epoch': 0.73}\n",
      "{'loss': 2.5158, 'grad_norm': 7.045246601104736, 'learning_rate': 1.3509085492999704e-05, 'epoch': 0.73}\n",
      "{'loss': 2.5465, 'grad_norm': 1.9308902025222778, 'learning_rate': 1.3360142984807863e-05, 'epoch': 0.73}\n",
      "{'loss': 2.7959, 'grad_norm': 2.6967742443084717, 'learning_rate': 1.3211200476616025e-05, 'epoch': 0.74}\n",
      "{'loss': 2.7182, 'grad_norm': 1.8520357608795166, 'learning_rate': 1.3062257968424188e-05, 'epoch': 0.74}\n",
      "{'loss': 2.5419, 'grad_norm': 2.0972740650177, 'learning_rate': 1.291331546023235e-05, 'epoch': 0.74}\n",
      "{'loss': 2.6273, 'grad_norm': 2.9495344161987305, 'learning_rate': 1.2764372952040512e-05, 'epoch': 0.74}\n",
      "{'loss': 2.585, 'grad_norm': 6.629668235778809, 'learning_rate': 1.2615430443848674e-05, 'epoch': 0.75}\n",
      "{'loss': 2.5586, 'grad_norm': 2.9534928798675537, 'learning_rate': 1.2466487935656837e-05, 'epoch': 0.75}\n",
      "{'loss': 2.6479, 'grad_norm': 3.705592393875122, 'learning_rate': 1.2317545427464999e-05, 'epoch': 0.75}\n",
      "{'loss': 2.7, 'grad_norm': 2.474174737930298, 'learning_rate': 1.2168602919273161e-05, 'epoch': 0.76}\n",
      "{'loss': 2.5846, 'grad_norm': 2.7699427604675293, 'learning_rate': 1.2019660411081324e-05, 'epoch': 0.76}\n",
      "{'loss': 2.5547, 'grad_norm': 2.1612765789031982, 'learning_rate': 1.1870717902889484e-05, 'epoch': 0.76}\n",
      "{'loss': 2.6182, 'grad_norm': 8.045963287353516, 'learning_rate': 1.1721775394697646e-05, 'epoch': 0.77}\n",
      "{'loss': 2.708, 'grad_norm': 2.2776896953582764, 'learning_rate': 1.1572832886505809e-05, 'epoch': 0.77}\n",
      "{'loss': 2.6987, 'grad_norm': 4.656325340270996, 'learning_rate': 1.1423890378313971e-05, 'epoch': 0.77}\n",
      "{'loss': 2.5995, 'grad_norm': 3.288193941116333, 'learning_rate': 1.1274947870122133e-05, 'epoch': 0.77}\n",
      "{'loss': 2.6719, 'grad_norm': 2.859459161758423, 'learning_rate': 1.1126005361930296e-05, 'epoch': 0.78}\n",
      "{'loss': 2.6061, 'grad_norm': 3.409187078475952, 'learning_rate': 1.0977062853738458e-05, 'epoch': 0.78}\n",
      "{'loss': 2.5264, 'grad_norm': 14.41512393951416, 'learning_rate': 1.0828120345546618e-05, 'epoch': 0.78}\n",
      "{'loss': 2.6824, 'grad_norm': 3.789630651473999, 'learning_rate': 1.067917783735478e-05, 'epoch': 0.79}\n",
      "{'loss': 2.6829, 'grad_norm': 2.3119254112243652, 'learning_rate': 1.0530235329162943e-05, 'epoch': 0.79}\n",
      "{'loss': 2.475, 'grad_norm': 1.7794653177261353, 'learning_rate': 1.0381292820971105e-05, 'epoch': 0.79}\n",
      "{'loss': 2.6174, 'grad_norm': 1.801451563835144, 'learning_rate': 1.0232350312779268e-05, 'epoch': 0.8}\n",
      "{'loss': 2.636, 'grad_norm': 2.2035768032073975, 'learning_rate': 1.008340780458743e-05, 'epoch': 0.8}\n",
      "{'loss': 2.5511, 'grad_norm': 1.9937914609909058, 'learning_rate': 9.93446529639559e-06, 'epoch': 0.8}\n",
      "{'loss': 2.4719, 'grad_norm': 2.9107446670532227, 'learning_rate': 9.785522788203753e-06, 'epoch': 0.8}\n",
      "{'loss': 2.5553, 'grad_norm': 3.1317803859710693, 'learning_rate': 9.636580280011915e-06, 'epoch': 0.81}\n",
      "{'loss': 2.6154, 'grad_norm': 1.8389272689819336, 'learning_rate': 9.487637771820077e-06, 'epoch': 0.81}\n",
      "{'loss': 2.6495, 'grad_norm': 3.082901954650879, 'learning_rate': 9.33869526362824e-06, 'epoch': 0.81}\n",
      "{'loss': 2.635, 'grad_norm': 1.4442152976989746, 'learning_rate': 9.189752755436402e-06, 'epoch': 0.82}\n",
      "{'loss': 2.4339, 'grad_norm': 6.398541450500488, 'learning_rate': 9.040810247244564e-06, 'epoch': 0.82}\n",
      "{'loss': 2.6884, 'grad_norm': 3.133336067199707, 'learning_rate': 8.891867739052725e-06, 'epoch': 0.82}\n",
      "{'loss': 2.5808, 'grad_norm': 2.005741834640503, 'learning_rate': 8.742925230860887e-06, 'epoch': 0.83}\n",
      "{'loss': 2.5609, 'grad_norm': 4.807345867156982, 'learning_rate': 8.59398272266905e-06, 'epoch': 0.83}\n",
      "{'loss': 2.5563, 'grad_norm': 2.1752374172210693, 'learning_rate': 8.445040214477212e-06, 'epoch': 0.83}\n",
      "{'loss': 2.7584, 'grad_norm': 1.822137713432312, 'learning_rate': 8.296097706285374e-06, 'epoch': 0.83}\n",
      "{'loss': 2.491, 'grad_norm': 2.033078908920288, 'learning_rate': 8.147155198093536e-06, 'epoch': 0.84}\n",
      "{'loss': 2.5159, 'grad_norm': 7.971832275390625, 'learning_rate': 7.998212689901698e-06, 'epoch': 0.84}\n",
      "{'loss': 2.696, 'grad_norm': 2.2647922039031982, 'learning_rate': 7.84927018170986e-06, 'epoch': 0.84}\n",
      "{'loss': 2.4633, 'grad_norm': 4.0579609870910645, 'learning_rate': 7.700327673518021e-06, 'epoch': 0.85}\n",
      "{'loss': 2.6626, 'grad_norm': 2.9114136695861816, 'learning_rate': 7.5513851653261844e-06, 'epoch': 0.85}\n",
      "{'loss': 2.6021, 'grad_norm': 1.9543499946594238, 'learning_rate': 7.402442657134347e-06, 'epoch': 0.85}\n",
      "{'loss': 2.5981, 'grad_norm': 2.8570988178253174, 'learning_rate': 7.253500148942509e-06, 'epoch': 0.85}\n",
      "{'loss': 2.6939, 'grad_norm': 2.0390634536743164, 'learning_rate': 7.104557640750671e-06, 'epoch': 0.86}\n",
      "{'loss': 2.4825, 'grad_norm': 1.7826275825500488, 'learning_rate': 6.955615132558832e-06, 'epoch': 0.86}\n",
      "{'loss': 2.51, 'grad_norm': 2.960944175720215, 'learning_rate': 6.806672624366994e-06, 'epoch': 0.86}\n",
      "{'loss': 2.5281, 'grad_norm': 3.0343310832977295, 'learning_rate': 6.6577301161751565e-06, 'epoch': 0.87}\n",
      "{'loss': 2.4933, 'grad_norm': 3.4551339149475098, 'learning_rate': 6.508787607983319e-06, 'epoch': 0.87}\n",
      "{'loss': 2.7368, 'grad_norm': 1.6543869972229004, 'learning_rate': 6.359845099791481e-06, 'epoch': 0.87}\n",
      "{'loss': 2.668, 'grad_norm': 3.874821901321411, 'learning_rate': 6.2109025915996425e-06, 'epoch': 0.88}\n",
      "{'loss': 2.634, 'grad_norm': 1.9879320859909058, 'learning_rate': 6.061960083407805e-06, 'epoch': 0.88}\n",
      "{'loss': 2.5938, 'grad_norm': 4.685815811157227, 'learning_rate': 5.913017575215967e-06, 'epoch': 0.88}\n",
      "{'loss': 2.6049, 'grad_norm': 3.0642943382263184, 'learning_rate': 5.7640750670241285e-06, 'epoch': 0.88}\n",
      "{'loss': 2.5762, 'grad_norm': 2.5032732486724854, 'learning_rate': 5.615132558832291e-06, 'epoch': 0.89}\n",
      "{'loss': 2.5376, 'grad_norm': 1.7716624736785889, 'learning_rate': 5.466190050640453e-06, 'epoch': 0.89}\n",
      "{'loss': 2.6978, 'grad_norm': 4.728063106536865, 'learning_rate': 5.317247542448615e-06, 'epoch': 0.89}\n",
      "{'loss': 2.5355, 'grad_norm': 1.8269068002700806, 'learning_rate': 5.168305034256777e-06, 'epoch': 0.9}\n",
      "{'loss': 2.6311, 'grad_norm': 4.775615692138672, 'learning_rate': 5.019362526064939e-06, 'epoch': 0.9}\n",
      "{'loss': 2.6924, 'grad_norm': 5.694470405578613, 'learning_rate': 4.870420017873101e-06, 'epoch': 0.9}\n",
      "{'loss': 2.5174, 'grad_norm': 2.38008975982666, 'learning_rate': 4.721477509681263e-06, 'epoch': 0.91}\n",
      "{'loss': 2.642, 'grad_norm': 2.5139424800872803, 'learning_rate': 4.572535001489425e-06, 'epoch': 0.91}\n",
      "{'loss': 2.5881, 'grad_norm': 2.181086778640747, 'learning_rate': 4.423592493297587e-06, 'epoch': 0.91}\n",
      "{'loss': 2.7471, 'grad_norm': 1.433463215827942, 'learning_rate': 4.274649985105749e-06, 'epoch': 0.91}\n",
      "{'loss': 2.6336, 'grad_norm': 1.7348612546920776, 'learning_rate': 4.125707476913911e-06, 'epoch': 0.92}\n",
      "{'loss': 2.7095, 'grad_norm': 2.384302854537964, 'learning_rate': 3.976764968722073e-06, 'epoch': 0.92}\n",
      "{'loss': 2.4975, 'grad_norm': 2.329397678375244, 'learning_rate': 3.827822460530236e-06, 'epoch': 0.92}\n",
      "{'loss': 2.4118, 'grad_norm': 7.710246562957764, 'learning_rate': 3.6788799523383975e-06, 'epoch': 0.93}\n",
      "{'loss': 2.6629, 'grad_norm': 2.7994139194488525, 'learning_rate': 3.5299374441465597e-06, 'epoch': 0.93}\n",
      "{'loss': 2.6206, 'grad_norm': 9.573014259338379, 'learning_rate': 3.380994935954722e-06, 'epoch': 0.93}\n",
      "{'loss': 2.5995, 'grad_norm': 1.6992292404174805, 'learning_rate': 3.2320524277628835e-06, 'epoch': 0.94}\n",
      "{'loss': 2.6094, 'grad_norm': 3.3268957138061523, 'learning_rate': 3.0831099195710457e-06, 'epoch': 0.94}\n",
      "{'loss': 2.5073, 'grad_norm': 3.511467456817627, 'learning_rate': 2.9341674113792076e-06, 'epoch': 0.94}\n",
      "{'loss': 2.6479, 'grad_norm': 2.1250650882720947, 'learning_rate': 2.78522490318737e-06, 'epoch': 0.94}\n",
      "{'loss': 2.4758, 'grad_norm': 4.702455520629883, 'learning_rate': 2.6362823949955317e-06, 'epoch': 0.95}\n",
      "{'loss': 2.6366, 'grad_norm': 2.982574462890625, 'learning_rate': 2.487339886803694e-06, 'epoch': 0.95}\n",
      "{'loss': 2.549, 'grad_norm': 6.4974751472473145, 'learning_rate': 2.338397378611856e-06, 'epoch': 0.95}\n",
      "{'loss': 2.5717, 'grad_norm': 2.1955947875976562, 'learning_rate': 2.1894548704200177e-06, 'epoch': 0.96}\n",
      "{'loss': 2.5938, 'grad_norm': 4.409570217132568, 'learning_rate': 2.04051236222818e-06, 'epoch': 0.96}\n",
      "{'loss': 2.5789, 'grad_norm': 3.4131245613098145, 'learning_rate': 1.8915698540363419e-06, 'epoch': 0.96}\n",
      "{'loss': 2.6244, 'grad_norm': 3.290452718734741, 'learning_rate': 1.7426273458445042e-06, 'epoch': 0.97}\n",
      "{'loss': 2.703, 'grad_norm': 1.6566240787506104, 'learning_rate': 1.593684837652666e-06, 'epoch': 0.97}\n",
      "{'loss': 2.5874, 'grad_norm': 4.4773759841918945, 'learning_rate': 1.4447423294608283e-06, 'epoch': 0.97}\n",
      "{'loss': 2.6758, 'grad_norm': 4.70908784866333, 'learning_rate': 1.2957998212689904e-06, 'epoch': 0.97}\n",
      "{'loss': 2.6263, 'grad_norm': 3.6563398838043213, 'learning_rate': 1.1468573130771522e-06, 'epoch': 0.98}\n",
      "{'loss': 2.504, 'grad_norm': 2.370412588119507, 'learning_rate': 9.979148048853143e-07, 'epoch': 0.98}\n",
      "{'loss': 2.4809, 'grad_norm': 1.6486990451812744, 'learning_rate': 8.489722966934764e-07, 'epoch': 0.98}\n",
      "{'loss': 2.5113, 'grad_norm': 2.250957489013672, 'learning_rate': 7.000297885016385e-07, 'epoch': 0.99}\n",
      "{'loss': 2.5717, 'grad_norm': 3.710604667663574, 'learning_rate': 5.510872803098004e-07, 'epoch': 0.99}\n",
      "{'loss': 2.6113, 'grad_norm': 3.519087553024292, 'learning_rate': 4.021447721179625e-07, 'epoch': 0.99}\n",
      "{'loss': 2.5172, 'grad_norm': 2.8835160732269287, 'learning_rate': 2.532022639261245e-07, 'epoch': 0.99}\n",
      "{'loss': 2.5378, 'grad_norm': 4.874938011169434, 'learning_rate': 1.0425975573428657e-07, 'epoch': 1.0}\n",
      "{'train_runtime': 1132.4075, 'train_samples_per_second': 23.718, 'train_steps_per_second': 2.964, 'train_loss': 2.6620341128524583, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3357, training_loss=2.6620341128524583, metrics={'train_runtime': 1132.4075, 'train_samples_per_second': 23.718, 'train_steps_per_second': 2.964, 'total_flos': 3710375651622912.0, 'train_loss': 2.6620341128524583, 'epoch': 0.9999255342914588})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Human: 考试有哪些技巧？\\n\\nAssistant: 考试技巧通常包括以下五方面：提前准备，认真准备，阅读考试说明材料，认真听考试说明材料，提高自己的写作能力，以及准备有纪律的考试细节。'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "ipt = \"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \"\n",
    "pipe(ipt, max_length=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
