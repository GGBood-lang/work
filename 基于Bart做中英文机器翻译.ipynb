{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9080986,"sourceType":"datasetVersion","datasetId":5478714}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import transformers\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.optim as optim\npd.set_option('display.max_colwidth', None)\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-01T12:23:54.893352Z","iopub.execute_input":"2024-08-01T12:23:54.894118Z","iopub.status.idle":"2024-08-01T12:23:54.900101Z","shell.execute_reply.started":"2024-08-01T12:23:54.894087Z","shell.execute_reply":"2024-08-01T12:23:54.899013Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"file_path=\"/kaggle/input/translate/.csv\"\ndf = pd.read_csv(file_path)\nprint(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:23:54.901563Z","iopub.execute_input":"2024-08-01T12:23:54.901836Z","iopub.status.idle":"2024-08-01T12:23:54.976680Z","shell.execute_reply.started":"2024-08-01T12:23:54.901812Z","shell.execute_reply":"2024-08-01T12:23:54.975629Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"                                                            Text  \\\n0                 It can be a very complicated thing, the ocean.   \n1  And it can be a very complicated thing, what human health is.   \n\n               Label  \n0      海洋是一个非常复杂的事物。  \n1  人类的健康也是一件非常复杂的事情。  \n","output_type":"stream"}]},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:23:54.978555Z","iopub.execute_input":"2024-08-01T12:23:54.978989Z","iopub.status.idle":"2024-08-01T12:23:54.987791Z","shell.execute_reply.started":"2024-08-01T12:23:54.978950Z","shell.execute_reply":"2024-08-01T12:23:54.986895Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-50')\nmodel = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:23:54.989152Z","iopub.execute_input":"2024-08-01T12:23:54.990055Z","iopub.status.idle":"2024-08-01T12:23:58.064287Z","shell.execute_reply.started":"2024-08-01T12:23:54.990028Z","shell.execute_reply":"2024-08-01T12:23:58.063487Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \nThe class this function is called from is 'MBartTokenizer'.\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_data(source_texts, target_texts, tokenizer, src_lang='en_XX', tgt_lang='zh_CN', max_length=128):\n    source_encodings = tokenizer(source_texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n    target_encodings = tokenizer(target_texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n    return source_encodings, target_encodings\n\n# 处理训练集数据\ntrain_source_texts = train_df['Text'].tolist()\ntrain_target_texts = train_df['Label'].tolist()\ntrain_source_encodings, train_target_encodings = tokenize_data(train_source_texts, train_target_texts, tokenizer)\n\n# 处理测试集数据\ntest_source_texts = test_df['Text'].tolist()\ntest_target_texts = test_df['Label'].tolist()\ntest_source_encodings, test_target_encodings = tokenize_data(test_source_texts, test_target_texts, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:23:58.066316Z","iopub.execute_input":"2024-08-01T12:23:58.066613Z","iopub.status.idle":"2024-08-01T12:24:05.809983Z","shell.execute_reply.started":"2024-08-01T12:23:58.066588Z","shell.execute_reply":"2024-08-01T12:24:05.808957Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, source_encodings, target_encodings):\n        self.source_encodings = source_encodings\n        self.target_encodings = target_encodings\n\n    def __len__(self):\n        return len(self.source_encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        source_item = {key: torch.tensor(val[idx]) for key, val in self.source_encodings.items()}\n        target_item = {key: torch.tensor(val[idx]) for key, val in self.target_encodings.items()}\n        return source_item, target_item\n\n# 创建数据加载器\ntrain_dataset = TranslationDataset(train_source_encodings, train_target_encodings)\ntest_dataset = TranslationDataset(test_source_encodings, test_target_encodings)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:24:05.811358Z","iopub.execute_input":"2024-08-01T12:24:05.811737Z","iopub.status.idle":"2024-08-01T12:24:05.820459Z","shell.execute_reply.started":"2024-08-01T12:24:05.811699Z","shell.execute_reply":"2024-08-01T12:24:05.819590Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:24:05.821757Z","iopub.execute_input":"2024-08-01T12:24:05.822111Z","iopub.status.idle":"2024-08-01T12:24:07.592941Z","shell.execute_reply.started":"2024-08-01T12:24:05.822061Z","shell.execute_reply":"2024-08-01T12:24:07.591993Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"MBartForConditionalGeneration(\n  (model): MBartModel(\n    (shared): Embedding(250054, 1024, padding_idx=1)\n    (encoder): MBartEncoder(\n      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): MBartDecoder(\n      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 3\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    \n    for source_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = source_batch['input_ids'].to(device)\n        attention_mask = source_batch['attention_mask'].to(device)\n        labels = target_batch['input_ids'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()\n        \n    avg_train_loss = total_train_loss / len(train_loader)\n    \n    model.eval()\n    total_eval_loss = 0\n    \n    with torch.no_grad():\n        for source_batch, target_batch in test_loader:\n            input_ids = source_batch['input_ids'].to(device)\n            attention_mask = source_batch['attention_mask'].to(device)\n            labels = target_batch['input_ids'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_eval_loss += loss.item()\n    \n    avg_eval_loss = total_eval_loss / len(test_loader)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {avg_train_loss}\")\n    print(f\"Eval Loss: {avg_eval_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:24:07.594175Z","iopub.execute_input":"2024-08-01T12:24:07.594455Z","iopub.status.idle":"2024-08-01T13:19:56.030474Z","shell.execute_reply.started":"2024-08-01T12:24:07.594432Z","shell.execute_reply":"2024-08-01T13:19:56.029434Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/2437398761.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  source_item = {key: torch.tensor(val[idx]) for key, val in self.source_encodings.items()}\n/tmp/ipykernel_33/2437398761.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  target_item = {key: torch.tensor(val[idx]) for key, val in self.target_encodings.items()}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\nTrain Loss: 0.6896660949803735\nEval Loss: 0.44413446116447447\nEpoch 2/3\nTrain Loss: 0.3520221918960505\nEval Loss: 0.4511162173748016\nEpoch 3/3\nTrain Loss: 0.24645656605236843\nEval Loss: 0.4883433943986893\n","output_type":"stream"}]},{"cell_type":"code","source":"model_save_path = \"/kaggle/working/mbart_translation_model\"\nmodel.save_pretrained(model_save_path)\n\n# 保存 tokenizer\ntokenizer_save_path = \"/kaggle/working/mbart_tokenizer\"\ntokenizer.save_pretrained(tokenizer_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T13:46:55.395247Z","iopub.execute_input":"2024-08-01T13:46:55.395637Z","iopub.status.idle":"2024-08-01T13:47:00.326890Z","shell.execute_reply.started":"2024-08-01T13:46:55.395602Z","shell.execute_reply":"2024-08-01T13:47:00.325935Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/mbart_tokenizer/tokenizer_config.json',\n '/kaggle/working/mbart_tokenizer/special_tokens_map.json',\n '/kaggle/working/mbart_tokenizer/sentencepiece.bpe.model',\n '/kaggle/working/mbart_tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"model = MBartForConditionalGeneration.from_pretrained(model_save_path)\ntokenizer = MBartTokenizer.from_pretrained(tokenizer_save_path)\nmodel.to(device)\ndef translate_text(text, tokenizer, model, src_lang='en_XX', tgt_lang='zh_CN', max_length=128):\n    # 准备输入数据\n    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n    \n    # 翻译\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            decoder_start_token_id=tokenizer.lang_code_to_id[tgt_lang],\n            max_length=max_length\n        )\n    \n    # 解码生成的文本\n    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return translated_text\n\n# 测试翻译\ntest_sentences = [\n    \"I want to eat delicious food.\",\n    \"I want to watch a movie.\",\n    \"What are you doing?\",\n    \"I want to become Superman.\"\n]\nfor sentence in test_sentences:\n    translated = translate_text(sentence, tokenizer, model)\n    print(f\"Input: {sentence}\")\n    print(f\"Translated: {translated}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T14:06:35.382904Z","iopub.execute_input":"2024-08-01T14:06:35.383588Z","iopub.status.idle":"2024-08-01T14:06:44.155939Z","shell.execute_reply.started":"2024-08-01T14:06:35.383556Z","shell.execute_reply":"2024-08-01T14:06:44.155025Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Input: I want to eat delicious food.\nTranslated: 我想要吃出美味的食物。\n\nInput: I want to watch a movie.\nTranslated: 我想要看个电影。\n\nInput: What are you doing?\nTranslated: 你在做什么?\n\nInput: I want to become Superman.\nTranslated: 我想要成为超人的。\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}